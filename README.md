# Hand-Sign-Recognition-Model

1.Problem:  Deaf and hearing-impaired individuals rely on sign language for communication, which creates a barrier for interaction with those who donâ€™t understand sign language.

2.Goal: Develop a real-time hand sign recognition system that translates American Sign Language (ASL) or other sign languages into written text or captions.
This system should be accurate, robust to variations in hand signs due to different signing styles, and work in various lighting conditions.

3.Dataset used: Word-Level American Sign Language

a) Content: Contains videos of 2,000 commonly used ASL signs. 

b) Focus: Designed for word-level recognition, meaning each video represents a single ASL sign.
![image](https://github.com/AkshatChoudhary410/Hand-Sign-Recognition-Model/assets/161162523/c36106fa-c22e-40e3-a5f0-dc91010fef12)
![image](https://github.com/AkshatChoudhary410/Hand-Sign-Recognition-Model/assets/161162523/a28293a9-b9c4-460a-99a5-cf903ac4b3cd)



4.Technologies Used -

a) MediaPipe

b) OpenCV

c) Keras

d) Dynamic Time Warping

Sign Language Recognition for 2 individuals in a frame-

![image](https://github.com/AkshatChoudhary410/Hand-Sign-Recognition-Model/assets/161162523/9fe0785d-85a2-49a9-b81a-73c742d40d32)




